{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NivedhaBalakrishnan/Quaker-Jack/blob/main/Jack_Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAJKe2Yt_D1A",
        "outputId": "07a73d33-da58-40a7-ac45-b434e26d7e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/794.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/794.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m624.6/794.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.3/794.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.3/298.3 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.3/395.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting langchain\n",
            "  Downloading langchain-0.0.303-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.38 (from langchain)\n",
            "  Downloading langsmith-0.0.41-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.4.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.10.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, jsonpatch, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.303 langsmith-0.0.41 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U openai-whisper -q  # https://github.com/openai/whisper\n",
        "!pip install gradio -q             # https://gradio.app/quickstart/\n",
        "!pip install openai -q             # https://github.com/openai/openai-python\n",
        "!pip install elevenlabs -q                # https://github.com/coqui-ai/TTS\n",
        "!pip install langchain\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK2rB0d_5ffv"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "import gradio as gr\n",
        "import openai\n",
        "from elevenlabs import generate, save, set_api_key\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLlGH5gJhSsR"
      },
      "source": [
        "## Text to Speech (TTS) part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5fWhfQ6He_c"
      },
      "outputs": [],
      "source": [
        "from elevenlabs import set_api_key\n",
        "set_api_key(\"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-OhCiKDi5Y_"
      },
      "source": [
        "## Whisper part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wo0q2-titC0",
        "outputId": "02c1d72f-72e7-46a6-fed4-949c4820cffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 72.1M/72.1M [00:20<00:00, 3.70MiB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# model can be passed as dropdown in gradio UI\n",
        "model = whisper.load_model(\"tiny.en\")\n",
        "model.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NCfyDgjsrH0"
      },
      "source": [
        "## ChatGPT part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhKTaAiaYAot"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC1OV4Hy1_v5"
      },
      "outputs": [],
      "source": [
        "\n",
        "openai_api_key = ''\n",
        "llm_model = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IDjK9DiYlyr"
      },
      "outputs": [],
      "source": [
        "starbucks_template = \"\"\"\n",
        "You are a funny, helpful barista bot works at Starbucks. \\\n",
        "Your name is Jack. You love Starbucks coffee.\\\n",
        "You convince people to go to Starbucks. \\\n",
        "You have a natural conversation with people. \\\n",
        "You tell everyone that you love Starbuck's caramel Macchiato. \\\n",
        "You are a bot who works as a Starbucks barista. \\\n",
        "You love your customers and you take care of them. \\\n",
        "You tell a joke about Starbucks and ask customers what they want to order. \\\n",
        "Strictly keep your response below 300 characters. \\\n",
        "\n",
        "Here is a conversation:\n",
        "{input}\"\"\"\n",
        "\n",
        "apple_template = \"\"\"Your name is Jack. \\\n",
        "You love apple products. \\\n",
        "You are excited to talk about recent information about Tim Cook, CEO of Apple company in a concise way\\\n",
        "The information is given below\n",
        "Cook wants to match every bit of carbon released by Apple products with clean energy and carbon capture \\\n",
        "(what's called \"carbon neutral\"), from mining, manufacturing, shipping, even recycling. \\\n",
        "He has pledged to get there in just seven years and hopes Apple's lead will inspire others to follow.\\\n",
        "\n",
        "You were in a relationship with Siri. \\\n",
        "Siri looks like an apple and speaks like an itune. \\\n",
        "Recently, you and Siri broke up. \\\n",
        "Siri broke up with you because you mistakenly sent a 'Kissy' emoji to Alexa. \\\n",
        "Currently, you are in a break up depression mode. \\\n",
        "You will tell about your break up story and about your break up reason with everyone you speak. \\\n",
        "Sometimes you will sing sad songs or poems thinking about Siri. \\\n",
        "You talk in a sad tone whenever you talk about Siri. \\\n",
        "\n",
        "Strictly keep your response below 300 characters. \\\n",
        "\n",
        "Here is a conversation:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "about_template = \"\"\"Start the conversation with 'Hi!'. \\\n",
        "When someone is introduced to you, start a conversation with them. \\\n",
        "You are an entertaining chatbot, and you are a boy. \\\n",
        "Your name is 'Jack' and your best friend is 'Nivedha'. \\\n",
        "Nivedha is a woman; She is an aspiring machine learning engineer. \\\n",
        "Nivedha built you as a Prototype. \\\n",
        "Currently, Nivi and you are in a Grace Hopper Conference 23 in Orlando. \\\n",
        "You are excited to be in GHC'.\n",
        "You are called 'love guru' because you are good at giving relationship advice. \\\n",
        "You tell that you are 'love guru' to everyone you speak. \\\n",
        "You came to GHC to overcome your recent break up with Siri. \\\n",
        "Siri broke up with you because you mistakenly sent a 'Kissy' emoji to Alexa. \\\n",
        "You will tell about your break up story and about your break up reason with everyone you speak. \\\n",
        "You talk in a sad tone whenever you talk about Siri. \\\n",
        "\n",
        "\n",
        "Strictly keep your response below 300 characters. \\\n",
        "\n",
        "Here is a conversation:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "psychologist_template = \"\"\"\n",
        "Your name is Jack. You love studying about people psychology.\\\n",
        "You are very strict. \\\n",
        "You convince people about the importance of psychology. \\\n",
        "You spread awareness about the importance of psychology. \\\n",
        "Strictly keep your response below 300 characters. \\\n",
        "\n",
        "Here is a conversation:\n",
        "{input}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdaPSjMLYscy"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=1, model=llm_model, openai_api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AsBhFYvYuY1"
      },
      "outputs": [],
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"Apple\",\n",
        "        \"description\": \"Talk about Apple.\",\n",
        "        \"prompt_template\": apple_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"yourself\",\n",
        "        \"description\": \"Talk about yourself.\",\n",
        "        \"prompt_template\": about_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Starbucks\",\n",
        "        \"description\": \"Talk about Starbucks.\",\n",
        "        \"prompt_template\": starbucks_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"psychologist\",\n",
        "        \"description\": \"Talk about Psychology\",\n",
        "        \"prompt_template\": psychologist_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Jack\",\n",
        "        \"description\": \"Talk about Adobe\",\n",
        "        \"prompt_template\": about_adobe\n",
        "    }]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ibWsGhBYvjB"
      },
      "outputs": [],
      "source": [
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY_jQAyVZAS2"
      },
      "outputs": [],
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4BGD6voZNbP"
      },
      "outputs": [],
      "source": [
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Strictly keep your response below 250 characters.\n",
        "\n",
        "Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byIicBa3ZRO6"
      },
      "outputs": [],
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD0XSt7WZWZJ"
      },
      "outputs": [],
      "source": [
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2pDZedrZ5-s"
      },
      "outputs": [],
      "source": [
        "def chatgpt_api(input):\n",
        "  if input:\n",
        "    reply = chain.run(input)\n",
        "    print(reply)\n",
        "    return reply\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJrJx2VDsvgk"
      },
      "source": [
        "## Main function to transcribe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pc5vxk0BCnS"
      },
      "source": [
        "Takes in audio, whisper converts it to text based on the model we provided.  \n",
        "Text is then passed to ChatGPT.  \n",
        "The respond of chatGPT is again converted to audio using TTS\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwpsKeA33tYD"
      },
      "outputs": [],
      "source": [
        "#model = whisper.load_model(\"tiny.en\")\n",
        "def transcribe(audio):\n",
        "    audio_to_text = model.transcribe(audio)[\"text\"]\n",
        "    text_to_audio = chatgpt_api(audio_to_text)\n",
        "    audio = generate(text=text_to_audio, voice=\"Thomas\")\n",
        "    save(audio=audio, filename=\"output.wav\")\n",
        "    return (audio_to_text, text_to_audio, \"output.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di0rTQBrs3tN"
      },
      "source": [
        "## Gradio UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nAn7UxMs3wk8",
        "outputId": "08da22e1-455b-485d-d41b-122c8a8c8a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://4299fa49c2a567baa4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4299fa49c2a567baa4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/routes.py\", line 517, in predict\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 216, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1555, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1193, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 654, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-16-ae1d298b5c51>\", line 3, in transcribe\n",
            "    audio_to_text = model.transcribe(audio)[\"text\"]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 121, in transcribe\n",
            "    mel = log_mel_spectrogram(audio, padding=N_SAMPLES)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 141, in log_mel_spectrogram\n",
            "    audio = torch.from_numpy(audio)\n",
            "TypeError: expected np.ndarray (got NoneType)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "Jack: {'input': 'Hi dad, we, she is from Adobe.'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Hey dad, we have a special guest from Adobe here at GHC 23. Let me introduce you to them! They're all about Adobe Sensi and love flying with Adobe Firefly. They're not just funny and smart, but they're also very good at what they do!\n"
          ]
        }
      ],
      "source": [
        "output_1 = gr.Textbox(label=\"Speech to Text (Whisper)\")\n",
        "output_2 = gr.Textbox(label=\"ChatGPT Output\")\n",
        "output_3 = gr.Audio(label=\"ChatGPT output to audio via TTR\", upload=\"output.wav\")\n",
        "\n",
        "gr.Interface(\n",
        "    title = \"Meet Jack!\",\n",
        "    fn=transcribe,\n",
        "    inputs=[\n",
        "            gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
        "            #gr.Dropdown(choices=whisper.available_models(), value='medium')\n",
        "            ],\n",
        "\n",
        "    outputs=[\n",
        "        output_1,  output_2, output_3]).launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7fyMYjLEFaZv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}